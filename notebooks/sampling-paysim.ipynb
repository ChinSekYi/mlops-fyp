{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7fefd1",
   "metadata": {},
   "source": [
    "# Samping raw data to simulate data used for dev, stag and prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7b973",
   "metadata": {},
   "source": [
    "split the chronological timeline into Dev (oldest) → Staging (recent past) → Prod (most recent). Each environment can then be used differently.\n",
    "\n",
    "split the 6.36M rows into:\n",
    "- Dev → ~70% oldest steps\n",
    "- Staging → ~15% middle steps\n",
    "- Prod → ~15% newest steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9e8a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe6d9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = Path(\"../data/raw\")\n",
    "CREDITCARD_PATH = RAW_PATH / \"paysim_data.csv\"\n",
    "\n",
    "df = pd.read_csv(CREDITCARD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e08e01e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud\n",
       "0    6354407\n",
       "1       8213\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].value_counts() #hightly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "769b182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 6362620 rows\n",
      "Fraud cases available: 8213\n",
      "Overall fraud rate: 0.0013\n",
      "\n",
      "Separated data:\n",
      "Fraud cases: 8213\n",
      "Non-fraud cases: 6354407\n",
      "\n",
      "Planned fraud distribution:\n",
      "Dev: 2000 fraud cases\n",
      "Staging: 1500 fraud cases\n",
      "Prod: 4713 fraud cases\n",
      "\n",
      "Actual fraud distribution:\n",
      "Dev: 2000 fraud cases\n",
      "Staging: 1500 fraud cases\n",
      "Prod: 4713 fraud cases\n",
      "\n",
      "Non-fraud distribution (50:50 balance):\n",
      "Dev: 2000 non-fraud cases\n",
      "Staging: 1500 non-fraud cases\n",
      "Prod: 4713 non-fraud cases\n",
      "\n",
      "Separated data:\n",
      "Fraud cases: 8213\n",
      "Non-fraud cases: 6354407\n",
      "\n",
      "Planned fraud distribution:\n",
      "Dev: 2000 fraud cases\n",
      "Staging: 1500 fraud cases\n",
      "Prod: 4713 fraud cases\n",
      "\n",
      "Actual fraud distribution:\n",
      "Dev: 2000 fraud cases\n",
      "Staging: 1500 fraud cases\n",
      "Prod: 4713 fraud cases\n",
      "\n",
      "Non-fraud distribution (50:50 balance):\n",
      "Dev: 2000 non-fraud cases\n",
      "Staging: 1500 non-fraud cases\n",
      "Prod: 4713 non-fraud cases\n",
      "\n",
      "=== FINAL DATASET SUMMARY (50:50 BALANCED) ===\n",
      "Dev: 4,000 total rows\n",
      "  - Fraud: 2,000 cases (0.5000 rate)\n",
      "  - Non-fraud: 2,000 cases\n",
      "\n",
      "Staging: 3,000 total rows\n",
      "  - Fraud: 1,500 cases (0.5000 rate)\n",
      "  - Non-fraud: 1,500 cases\n",
      "\n",
      "Prod: 6,355,620 total rows\n",
      "  - Fraud: 4,713 cases (0.0007 rate)\n",
      "  - Non-fraud: 6,350,907 cases\n",
      "\n",
      "Total fraud cases used: 8,213/8,213\n",
      "\n",
      "=== BALANCE VERIFICATION ===\n",
      "Dev fraud rate: 0.5000 (should be 0.5000)\n",
      "Staging fraud rate: 0.5000 (should be 0.5000)\n",
      "Prod fraud rate: 0.0007 (should be 0.5000)\n",
      "\n",
      "=== FINAL DATASET SUMMARY (50:50 BALANCED) ===\n",
      "Dev: 4,000 total rows\n",
      "  - Fraud: 2,000 cases (0.5000 rate)\n",
      "  - Non-fraud: 2,000 cases\n",
      "\n",
      "Staging: 3,000 total rows\n",
      "  - Fraud: 1,500 cases (0.5000 rate)\n",
      "  - Non-fraud: 1,500 cases\n",
      "\n",
      "Prod: 6,355,620 total rows\n",
      "  - Fraud: 4,713 cases (0.0007 rate)\n",
      "  - Non-fraud: 6,350,907 cases\n",
      "\n",
      "Total fraud cases used: 8,213/8,213\n",
      "\n",
      "=== BALANCE VERIFICATION ===\n",
      "Dev fraud rate: 0.5000 (should be 0.5000)\n",
      "Staging fraud rate: 0.5000 (should be 0.5000)\n",
      "Prod fraud rate: 0.0007 (should be 0.5000)\n"
     ]
    }
   ],
   "source": [
    "# Perfect 50:50 balanced sampling strategy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Total data: {len(df)} rows\")\n",
    "print(f\"Fraud cases available: {df['isFraud'].sum()}\")\n",
    "print(f\"Overall fraud rate: {df['isFraud'].mean():.4f}\")\n",
    "\n",
    "# Separate fraud and non-fraud cases\n",
    "fraud_cases = df[df['isFraud'] == 1].copy()\n",
    "non_fraud_cases = df[df['isFraud'] == 0].copy()\n",
    "\n",
    "print(f\"\\nSeparated data:\")\n",
    "print(f\"Fraud cases: {len(fraud_cases)}\")\n",
    "print(f\"Non-fraud cases: {len(non_fraud_cases)}\")\n",
    "\n",
    "# Strategy: 50:50 balance for all environments\n",
    "# Distribute fraud cases across environments\n",
    "fraud_dev_size = 2000      # 2k fraud cases for dev\n",
    "fraud_staging_size = 1500  # 1.5k fraud cases for staging  \n",
    "fraud_prod_size = len(fraud_cases) - fraud_dev_size - fraud_staging_size  # Remaining ~4.7k fraud cases\n",
    "\n",
    "print(f\"\\nPlanned fraud distribution:\")\n",
    "print(f\"Dev: {fraud_dev_size} fraud cases\")\n",
    "print(f\"Staging: {fraud_staging_size} fraud cases\")\n",
    "print(f\"Prod: {fraud_prod_size} fraud cases\")\n",
    "\n",
    "# Split fraud cases\n",
    "fraud_dev, fraud_temp = train_test_split(\n",
    "    fraud_cases, \n",
    "    train_size=fraud_dev_size, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fraud_staging, fraud_prod = train_test_split(\n",
    "    fraud_temp, \n",
    "    train_size=fraud_staging_size, \n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "print(f\"\\nActual fraud distribution:\")\n",
    "print(f\"Dev: {len(fraud_dev)} fraud cases\")\n",
    "print(f\"Staging: {len(fraud_staging)} fraud cases\") \n",
    "print(f\"Prod: {len(fraud_prod)} fraud cases\")\n",
    "\n",
    "# For 50:50 balance, non-fraud cases = fraud cases\n",
    "non_fraud_dev_size = len(fraud_dev)      # 2000 non-fraud\n",
    "non_fraud_staging_size = len(fraud_staging)  # 1500 non-fraud\n",
    "non_fraud_prod_size = len(fraud_prod)    # ~4713 non-fraud\n",
    "\n",
    "print(f\"\\nNon-fraud distribution (50:50 balance):\")\n",
    "print(f\"Dev: {non_fraud_dev_size} non-fraud cases\")\n",
    "print(f\"Staging: {non_fraud_staging_size} non-fraud cases\")\n",
    "print(f\"Prod: {non_fraud_prod_size} non-fraud cases\")\n",
    "\n",
    "# Split non-fraud cases\n",
    "non_fraud_dev, non_fraud_temp = train_test_split(\n",
    "    non_fraud_cases, \n",
    "    train_size=non_fraud_dev_size, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "non_fraud_staging, non_fraud_prod = train_test_split(\n",
    "    non_fraud_temp, \n",
    "    train_size=non_fraud_staging_size, \n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "# Combine fraud + non-fraud for each environment (50:50 balance)\n",
    "dev_df = pd.concat([fraud_dev, non_fraud_dev], ignore_index=True)\n",
    "staging_df = pd.concat([fraud_staging, non_fraud_staging], ignore_index=True)\n",
    "prod_df = pd.concat([fraud_prod, non_fraud_prod], ignore_index=True)\n",
    "\n",
    "# Shuffle each dataset\n",
    "dev_df = dev_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "staging_df = staging_df.sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "prod_df = prod_df.sample(frac=1, random_state=44).reset_index(drop=True)\n",
    "\n",
    "# Save files\n",
    "dev_df.to_csv(RAW_PATH / \"paysim-dev.csv\", index=False)\n",
    "staging_df.to_csv(RAW_PATH / \"paysim-staging.csv\", index=False)\n",
    "prod_df.to_csv(RAW_PATH / \"paysim-prod.csv\", index=False)\n",
    "\n",
    "print(f\"\\n=== FINAL DATASET SUMMARY (50:50 BALANCED) ===\")\n",
    "print(f\"Dev: {len(dev_df):,} total rows\")\n",
    "print(f\"  - Fraud: {dev_df['isFraud'].sum():,} cases ({dev_df['isFraud'].mean():.4f} rate)\")\n",
    "print(f\"  - Non-fraud: {(dev_df['isFraud'] == 0).sum():,} cases\")\n",
    "\n",
    "print(f\"\\nStaging: {len(staging_df):,} total rows\") \n",
    "print(f\"  - Fraud: {staging_df['isFraud'].sum():,} cases ({staging_df['isFraud'].mean():.4f} rate)\")\n",
    "print(f\"  - Non-fraud: {(staging_df['isFraud'] == 0).sum():,} cases\")\n",
    "\n",
    "print(f\"\\nProd: {len(prod_df):,} total rows\")\n",
    "print(f\"  - Fraud: {prod_df['isFraud'].sum():,} cases ({prod_df['isFraud'].mean():.4f} rate)\")\n",
    "print(f\"  - Non-fraud: {(prod_df['isFraud'] == 0).sum():,} cases\")\n",
    "\n",
    "print(f\"\\nTotal fraud cases used: {dev_df['isFraud'].sum() + staging_df['isFraud'].sum() + prod_df['isFraud'].sum():,}/{len(fraud_cases):,}\")\n",
    "\n",
    "# Verify perfect balance\n",
    "print(f\"\\n=== BALANCE VERIFICATION ===\")\n",
    "print(f\"Dev fraud rate: {dev_df['isFraud'].mean():.4f} (should be 0.5000)\")\n",
    "print(f\"Staging fraud rate: {staging_df['isFraud'].mean():.4f} (should be 0.5000)\")\n",
    "print(f\"Prod fraud rate: {prod_df['isFraud'].mean():.4f} (should be 0.5000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960b83d",
   "metadata": {},
   "source": [
    "df = df.sample(n=30000, random_state=42)  # Take random 30k rows for speed\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d7d40",
   "metadata": {},
   "source": [
    "# Best of both worlds: small dev, balanced classes, time-aware\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Overall fraud rate for reference\n",
    "overall_fraud_rate = df['isFraud'].mean()\n",
    "print(f\"Overall fraud rate: {overall_fraud_rate:.4f}\")\n",
    "\n",
    "# Dev: Small stratified sample (fast development)\n",
    "dev_df, remaining_df = train_test_split(\n",
    "    df, \n",
    "    train_size=2000,  # Small for fast iteration\n",
    "    stratify=df['isFraud'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split remaining into staging and prod\n",
    "staging_df, prod_df = train_test_split(\n",
    "    remaining_df, \n",
    "    train_size=5000,  # Medium for validation\n",
    "    stratify=remaining_df['isFraud'], \n",
    "    random_state=43\n",
    ")\n",
    "\n",
    "# Save files\n",
    "dev_df.to_csv(RAW_PATH / \"paysim-dev.csv\", index=False)\n",
    "staging_df.to_csv(RAW_PATH / \"paysim-staging.csv\", index=False)\n",
    "prod_df.to_csv(RAW_PATH / \"paysim-prod.csv\", index=False)\n",
    "\n",
    "print(f\"Dev: {len(dev_df)} rows (fast development)\")\n",
    "print(f\"Staging: {len(staging_df)} rows (validation)\")  \n",
    "print(f\"Prod: {len(prod_df)} rows (production)\")\n",
    "print(f\"Fraud rates: {dev_df['isFraud'].mean():.4f}, {staging_df['isFraud'].mean():.4f}, {prod_df['isFraud'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf16ba",
   "metadata": {},
   "source": [
    "To avoid data leakage & keep fraud class balance:\n",
    "Use stratified sampling on the target column (Class) so the fraud ratio stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27bb44",
   "metadata": {},
   "source": [
    "# Check unique step values (time simulation)\n",
    "print(\"Step range:\", df[\"step\"].min(), \"to\", df[\"step\"].max())\n",
    "\n",
    "# Split by quantiles on step\n",
    "q1 = df[\"step\"].quantile(0.70)   # 70% cutoff\n",
    "q2 = df[\"step\"].quantile(0.85)   # 85% cutoff\n",
    "\n",
    "dev_df = df[df[\"step\"] <= q1].copy()\n",
    "staging_df = df[(df[\"step\"] > q1) & (df[\"step\"] <= q2)].copy()\n",
    "prod_df = df[df[\"step\"] > q2].copy()\n",
    "\n",
    "dev_df.to_csv(RAW_PATH / \"paysim-dev.csv\", index=False)\n",
    "staging_df.to_csv(RAW_PATH / \"paysim-staging.csv\", index=False)\n",
    "prod_df.to_csv(RAW_PATH / \"paysim-prod.csv\", index=False)\n",
    "\n",
    "print(f\"Dev: {len(dev_df)} rows, Staging: {len(staging_df)} rows, Prod: {len(prod_df)} rows\")\n",
    "print(\"Fraud rates:\", dev_df['isFraud'].mean(), staging_df['isFraud'].mean(), prod_df['isFraud'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1638de29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud rates across environments:\n",
      "Dev: 0.5000\n",
      "Staging: 0.5000\n",
      "Prod: 0.0007\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraud rates across environments:\")\n",
    "print(f\"Dev: {dev_df['isFraud'].mean():.4f}\")\n",
    "print(f\"Staging: {staging_df['isFraud'].mean():.4f}\")  \n",
    "print(f\"Prod: {prod_df['isFraud'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d031d68f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4031129170.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[42], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    dvc add data/raw/paysim-prod.csv\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# then push the data to s3 via dvc\n",
    "eg\n",
    "dvc add data/raw/paysim-prod.csv\n",
    "dvc push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
